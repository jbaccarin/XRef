{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    import java.util.Scanner;\\n \\n public class En...\n",
      "1    import java.util.Scanner;\\n \\n public class Po...\n",
      "2    import java.util.Scanner;\\n \\n public class Po...\n",
      "3    import java.util.Scanner;\\n \\n public class No...\n",
      "4    #include<cstdio>\\n #include<cstdlib>\\n \\n #def...\n",
      "Name: flines, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### read data\n",
    "data = pd.read_csv(\"/Users/timcerta/code/jbaccarin/xref/raw_data/gcj2008.csv\")\n",
    "data = data[data.flines.notna()]\n",
    "data_small = data[:5]\n",
    "print(data_small[\"flines\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>round</th>\n",
       "      <th>username</th>\n",
       "      <th>task</th>\n",
       "      <th>solution</th>\n",
       "      <th>file</th>\n",
       "      <th>full_path</th>\n",
       "      <th>flines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>32002</td>\n",
       "      <td>MikeSeibert</td>\n",
       "      <td>24445</td>\n",
       "      <td>0</td>\n",
       "      <td>EndlessKnight.java</td>\n",
       "      <td>gcj/2008/32002/MikeSeibert/24445/0/extracted/E...</td>\n",
       "      <td>import java.util.Scanner;\\n \\n public class En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>32002</td>\n",
       "      <td>MikeSeibert</td>\n",
       "      <td>24444</td>\n",
       "      <td>1</td>\n",
       "      <td>Pockets.java</td>\n",
       "      <td>gcj/2008/32002/MikeSeibert/24444/1/extracted/P...</td>\n",
       "      <td>import java.util.Scanner;\\n \\n public class Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>32002</td>\n",
       "      <td>MikeSeibert</td>\n",
       "      <td>24444</td>\n",
       "      <td>0</td>\n",
       "      <td>Pockets.java</td>\n",
       "      <td>gcj/2008/32002/MikeSeibert/24444/0/extracted/P...</td>\n",
       "      <td>import java.util.Scanner;\\n \\n public class Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>32002</td>\n",
       "      <td>MikeSeibert</td>\n",
       "      <td>24446</td>\n",
       "      <td>0</td>\n",
       "      <td>NoCheating.java</td>\n",
       "      <td>gcj/2008/32002/MikeSeibert/24446/0/extracted/N...</td>\n",
       "      <td>import java.util.Scanner;\\n \\n public class No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>32002</td>\n",
       "      <td>bhamrick</td>\n",
       "      <td>24445</td>\n",
       "      <td>0</td>\n",
       "      <td>D.cpp</td>\n",
       "      <td>gcj/2008/32002/bhamrick/24445/0/extracted/D.cpp</td>\n",
       "      <td>#include&lt;cstdio&gt;\\n #include&lt;cstdlib&gt;\\n \\n #def...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  year  round     username   task  solution                file  \\\n",
       "0           0  2008  32002  MikeSeibert  24445         0  EndlessKnight.java   \n",
       "1           1  2008  32002  MikeSeibert  24444         1        Pockets.java   \n",
       "2           2  2008  32002  MikeSeibert  24444         0        Pockets.java   \n",
       "3           3  2008  32002  MikeSeibert  24446         0     NoCheating.java   \n",
       "4           4  2008  32002     bhamrick  24445         0               D.cpp   \n",
       "\n",
       "                                           full_path  \\\n",
       "0  gcj/2008/32002/MikeSeibert/24445/0/extracted/E...   \n",
       "1  gcj/2008/32002/MikeSeibert/24444/1/extracted/P...   \n",
       "2  gcj/2008/32002/MikeSeibert/24444/0/extracted/P...   \n",
       "3  gcj/2008/32002/MikeSeibert/24446/0/extracted/N...   \n",
       "4    gcj/2008/32002/bhamrick/24445/0/extracted/D.cpp   \n",
       "\n",
       "                                              flines  \n",
       "0  import java.util.Scanner;\\n \\n public class En...  \n",
       "1  import java.util.Scanner;\\n \\n public class Po...  \n",
       "2  import java.util.Scanner;\\n \\n public class Po...  \n",
       "3  import java.util.Scanner;\\n \\n public class No...  \n",
       "4  #include<cstdio>\\n #include<cstdlib>\\n \\n #def...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49072 entries, 0 to 49096\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  49072 non-null  int64 \n",
      " 1   year        49072 non-null  int64 \n",
      " 2   round       49072 non-null  int64 \n",
      " 3   username    49072 non-null  object\n",
      " 4   task        49072 non-null  int64 \n",
      " 5   solution    49072 non-null  int64 \n",
      " 6   file        49072 non-null  object\n",
      " 7   full_path   49072 non-null  object\n",
      " 8   flines      49072 non-null  object\n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import java.util.Scanner;\\n \\n public class EndlessKnight\\n {\\n \\tpublic static void main(String[] args)\\n \\t{\\n \\t\\tScanner in = new Scanner(System.in);\\n \\t\\tint numCases = in.nextInt();\\n \\t\\tfor (int i = 0; i < numCases; i++)\\n \\t\\t\\tdoCase(i + 1, in);\\n \\t}\\n \\n \\tprivate static void doCase(int caseNum, Scanner in)\\n \\t{\\n \\t\\tint H = in.nextInt();\\n \\t\\tint W = in.nextInt();\\n \\t\\tint R = in.nextInt();\\n \\t\\t\\n \\t\\tint[][] memoized = new int[W][H];\\n \\t\\tfor (int[] arr : memoized)\\n \\t\\t\\tfor (int i = 0; i < arr.length; i++)\\n \\t\\t\\t\\tarr[i] = -1;\\n \\t\\t\\n \\t\\tfor (int i = 0; i < R; i++)\\n \\t\\t{\\n \\t\\t\\tint rockY = in.nextInt();\\n \\t\\t\\tint rockX = in.nextInt();\\n \\t\\t\\tmemoized[W - rockX][H - rockY] = 0;\\n \\t\\t}\\n \\t\\t\\n \\t\\tint total = findMoves(W - 1, H - 1, memoized);\\n \\t\\t\\n \\t\\tSystem.out.println(\"Case #\" + caseNum + \": \" + total);\\n \\t}\\n \\n \\tprivate static int findMoves(int i, int j, int[][] memoized)\\n \\t{\\n \\t\\tif (i == 0 && j == 0)\\n \\t\\t\\treturn 1;\\n \\t\\t\\n \\t\\tif (i < 1 || j < 1)\\n \\t\\t\\treturn 0;\\n \\t\\t\\n \\t\\tif (memoized[i][j] != -1)\\n \\t\\t\\treturn memoized[i][j];\\n \\t\\t\\n \\t\\tmemoized[i][j] = (findMoves(i - 1, j - 2, memoized) + findMoves(i - 2, j - 1, memoized)) % 10007;\\n \\t\\treturn memoized[i][j];\\n \\t}\\n }\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = data.flines[0]\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [45], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m count_vectorizer \u001b[39m=\u001b[39m CountVectorizer(ngram_range\u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[39m#X = count_vectorizer.fit_transform(data.flines).toarray()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#X\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(count_vectorizer\u001b[39m.\u001b[39;49mfit_transform(data\u001b[39m.\u001b[39;49mflines)\u001b[39m.\u001b[39mtoarray(),\n\u001b[1;32m     10\u001b[0m                  columns \u001b[39m=\u001b[39m count_vectorizer\u001b[39m.\u001b[39mget_feature_names_out())\n\u001b[1;32m     11\u001b[0m X\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:118\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    116\u001b[0m             doc \u001b[39m=\u001b[39m ngrams(doc, stop_words)\n\u001b[1;32m    117\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m             doc \u001b[39m=\u001b[39m ngrams(doc)\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:262\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[0;34m(self, tokens, stop_words)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39m# bind method outside of loop to reduce overhead\u001b[39;00m\n\u001b[1;32m    261\u001b[0m tokens_append \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mappend\n\u001b[0;32m--> 262\u001b[0m space_join \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin\n\u001b[1;32m    264\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(min_n, \u001b[39mmin\u001b[39m(max_n \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, n_original_tokens \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    265\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_original_tokens \u001b[39m-\u001b[39m n \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Word frequencies\n",
    "# different, we need to do this on the wole dataset. \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range= (1, 2))\n",
    "#X = count_vectorizer.fit_transform(data.flines).toarray()\n",
    "#X\n",
    "\n",
    "X = pd.DataFrame(count_vectorizer.fit_transform(data.flines).toarray(),\n",
    "                 columns = count_vectorizer.get_feature_names_out())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate n-grams\n",
    "\n",
    "def word2ngrams(text, n=3, exact=True):\n",
    "    \"\"\" Convert text into character ngrams. \"\"\"\n",
    "    return [\"\".join(j) for j in zip(*[text[i:] for i in range(n)])]\n",
    "\n",
    "ngrams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "for count, gram in enumerate(ngrams):\n",
    "    gram = word2ngrams(text = code, n = count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Functions for data preprocessing/ feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "def create_metrics(code):\n",
    "        \"\"\"\n",
    "        Creates metrics needed for modeling\n",
    "        :return: returns an Array with metrics calculated from the source code\n",
    "        \"\"\"\n",
    "        # TODO: assign vars for redundant code\n",
    "        # TODO: self.var for all variable assignment\n",
    "\n",
    "        # Number of characters in code/ length of code\n",
    "        code_len = len(code)\n",
    "\n",
    "        # Number of characters in line\n",
    "        code_tokens = code.split(\"\\n\")\n",
    "        nchar_in_line = mean([code_len for token in code_tokens])\n",
    "\n",
    "        # Lines of code / file length in chars\n",
    "        n_lines = code.count('\\n')\n",
    "\n",
    "        # Create n-grams\n",
    "        #unigram = word2ngrams(text = self.code, n = 1)\n",
    "        #bigram = word2ngrams(text = self.code, n = 2)\n",
    "        #trigram = word2ngrams(text = self.code, n = 3)\n",
    "\n",
    "        # Number of words in the text / file length in characters\n",
    "        word_len_ratio = len(code.split())/code_len\n",
    "\n",
    "        # Average number of words per line / file length in characters\n",
    "        pre = (code.split('\\n'))\n",
    "        words_list = [x.split() for i, x in enumerate(pre)]\n",
    "        words_count = [len(words_list[i]) for i, x in enumerate(pre)]\n",
    "        avg_words_per_line = np.mean(words_count)\n",
    "\n",
    "        # Number of whitespace / file length in characters\n",
    "        whitespace_ratio = code.count(' ')/code_len\n",
    "\n",
    "        # Number of line breaks / file length in characters\n",
    "        linebreak_ratio = code.count('\\n')/len(code)\n",
    "\n",
    "        # Number of indentations / file length in characters\n",
    "        indent_ratio = code.count('\\t')/code_len\n",
    "\n",
    "        # Number of upper_case words / file length in characters\n",
    "        uppercase_ratio = sum(1 for char in code if char.isupper())/code_len\n",
    "\n",
    "        # Number of lower_case words / file length in characters\n",
    "        lowercase_ratio = sum(1 for char in code if char.islower())/code_len\n",
    "\n",
    "        # Number of punctuation symbols / file length in characters\n",
    "        punctuation_count = Counter(punc for line in code for punc in line if punc in punctuation)\n",
    "        punctuation_count_ratio = sum(punctuation_count.values())/ code_len\n",
    "\n",
    "        # Average length of words(exluding punctuation, excluding single letter or number)\n",
    "        code_no_punc = code\n",
    "        for punc in punctuation:\n",
    "            code_no_punc = code_no_punc.replace(punc, '')\n",
    "        code_no_single_char = ' '.join([w for w in code_no_punc.split() if len(w)>1])\n",
    "        list_len_words = [len(x) for i, x in enumerate(code_no_single_char.split())]\n",
    "        avg_char_per_word = np.mean(list_len_words)\n",
    "\n",
    "        #Keywords count ('if', 'else', 'while', 'for', 'in', 'elif', 'or', 'not', 'with', 'and', 'is')\n",
    "        keyword_count_ratio = sum([code.count(x) for x in ['if', 'else', 'while', 'for', 'in', 'elif', 'or', 'not', 'with', 'and', 'is'] ]) / code_len\n",
    "\n",
    "        # Return all metrics as array\n",
    "        return np.array([code_len,\n",
    "                        nchar_in_line,\n",
    "                        n_lines,\n",
    "                        word_len_ratio,\n",
    "                        avg_words_per_line,\n",
    "                        whitespace_ratio,\n",
    "                        linebreak_ratio,\n",
    "                        indent_ratio,\n",
    "                        uppercase_ratio,\n",
    "                        lowercase_ratio,\n",
    "                        punctuation_count_ratio,\n",
    "                        avg_char_per_word,\n",
    "                        keyword_count_ratio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1118.0, 1118.0, 50.0, 0.16279069767441862, 3....\n",
       "1    [2995.0, 2995.0, 152.0, 0.14156928213689482, 2...\n",
       "2    [2995.0, 2995.0, 152.0, 0.14156928213689482, 2...\n",
       "3    [1716.0, 1716.0, 74.0, 0.1754079254079254, 4.0...\n",
       "4    [900.0, 900.0, 41.0, 0.12111111111111111, 2.59...\n",
       "Name: flines, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = data_small.flines.apply(lambda x:create_metrics(code = x))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create Count Vectorization for text + ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(flines, analyzer = 'word', ngram_range = (1, 1), min_df = 3):\n",
    "    \"\"\"\n",
    "    Apply count vectorizer to a given column (flines)\n",
    "    :param analyzer:\n",
    "        if 'word': vectorization happens with word n-grams\n",
    "        if 'char': vectorization happens with character n-grams, padding the empty space to the tokens\n",
    "        if 'char_wb': creates character n-grams only from text inside word boundaries\n",
    "    :param ngram_range: the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.\n",
    "        examples: (1, 1) extracts unigrams, (1, 2) extracts unigrams and bigrams\n",
    "    :param min_df: remove terms with frequency lower than threshold\n",
    "    :return: array of count_frequencies\n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer(analyzer = analyzer, ngram_range = ngram_range)\n",
    "    X = count_vectorizer.fit_transform(flines).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply count_vectorizer to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  0,  0,  1,  3,  0,  0,  0,  0,  0,  1,  2,  0,  0,\n",
       "         1,  0,  0,  0,  0,  0,  0,  2,  0,  0,  1,  0,  0,  0,  0,  4,\n",
       "         0,  4,  0,  0,  0,  3,  1, 10,  0, 18,  0,  0,  1,  1,  1,  0,\n",
       "         0,  0, 11,  0,  0,  2,  0,  0,  6,  0,  0,  2,  0,  0,  0,  0,\n",
       "         1,  0,  0,  0,  1,  2,  0,  2,  0,  4,  0,  2,  2,  4,  0,  0,\n",
       "         3,  0,  1,  0,  0,  2,  2,  0,  0,  1,  2,  0,  0,  0],\n",
       "       [ 0,  0, 14, 20,  1,  1,  0,  0,  6, 16,  0,  0, 16,  2,  1,  1,\n",
       "         1,  0,  0,  0,  0, 12, 11,  2,  0,  6,  0,  1,  0,  0,  0,  0,\n",
       "         0, 10,  0,  0,  0,  8,  1,  8,  0, 22,  0,  0,  1,  1,  1,  0,\n",
       "         0,  0,  0,  0,  0,  7,  0,  1,  3,  0,  7,  2, 10,  4,  4,  0,\n",
       "         3,  4,  4,  4,  3,  1,  1,  2,  2,  0,  0,  0,  0,  4,  0,  6,\n",
       "         2,  0,  2,  2,  4,  4,  0,  6,  0,  1,  2,  6,  4,  4],\n",
       "       [ 0,  0, 14, 20,  1,  1,  0,  0,  6, 16,  0,  0, 16,  2,  1,  1,\n",
       "         1,  0,  0,  0,  0, 12, 11,  2,  0,  6,  0,  1,  0,  0,  0,  0,\n",
       "         0, 10,  0,  0,  0,  8,  1,  8,  0, 22,  0,  0,  1,  1,  1,  0,\n",
       "         0,  0,  0,  0,  0,  7,  0,  1,  3,  0,  7,  2, 10,  4,  4,  0,\n",
       "         3,  4,  4,  4,  3,  1,  1,  2,  2,  0,  0,  0,  0,  4,  0,  6,\n",
       "         2,  0,  2,  2,  4,  4,  0,  6,  0,  1,  2,  6,  4,  4],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  2,  1,  0,  9,  3,  1,  2,  0,  1,\n",
       "         1,  1,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  7,  0,  0,  0,  7,  1,  8,  0, 29,  2,  3,  1,  0,  1,  8,\n",
       "         0,  0,  0,  0,  0,  3,  6,  1,  3,  1,  0,  2,  0,  0,  0,  8,\n",
       "         1,  0,  0,  0,  1,  3,  0,  2,  0,  4,  0,  0,  0,  4, 12,  0,\n",
       "         4,  0,  2,  0,  0,  2,  0,  0,  0,  1,  2,  0,  0,  0],\n",
       "       [ 1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
       "         0,  0,  1,  1,  3,  0,  0,  0,  9,  0,  0,  0,  2,  1,  5,  0,\n",
       "         2,  6,  3,  1,  3,  2,  0,  1,  2, 10,  0,  0,  0,  0,  1,  0,\n",
       "         4,  4,  0,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1,  0,  0,  0,  0,  0,  0,  0,  0,  1,  5,  0,  0,  0,  0,  0,\n",
       "         0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vec = count_vectorizer(flines = data_small[\"flines\"])\n",
    "word_count_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply count_vectorizer to n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 181,   1, ...,   0,   0,   1],\n",
       "       [  1, 423,   4, ...,   0,   1,   5],\n",
       "       [  1, 423,   4, ...,   0,   1,   5],\n",
       "       [  1, 300,   7, ...,   1,   1,   2],\n",
       "       [  1, 108,   2, ...,   0,   0,   1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_count_vec = count_vectorizer(flines = data_small[\"flines\"], analyzer = \"char\", ngram_range = (1, 3))\n",
    "ngram_count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (5,94) into shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[39m.\u001b[39;49mappend([metrics, word_count_vec, ngram_count_vec], values \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/numpy/lib/function_base.py:5438\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5389\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_append_dispatcher)\n\u001b[1;32m   5390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappend\u001b[39m(arr, values, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   5391\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5392\u001b[0m \u001b[39m    Append values to the end of an array.\u001b[39;00m\n\u001b[1;32m   5393\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5436\u001b[0m \n\u001b[1;32m   5437\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5438\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(arr)\n\u001b[1;32m   5439\u001b[0m     \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5440\u001b[0m         \u001b[39mif\u001b[39;00m arr\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (5,94) into shape (5,)"
     ]
    }
   ],
   "source": [
    "np.append([metrics, word_count_vec, ngram_count_vec], values = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a747c0bd7ae814acf2eeb07818ad520afe16d75094092469c816bf0341d8867a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
