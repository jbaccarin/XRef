{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "### read data\n",
    "data = pd.read_csv(\"/Users/timcerta/code/jbaccarin/xref/raw_data/gcj2008.csv\")\n",
    "# Remove NAs\n",
    "data = data.dropna()\n",
    "# Remove code with less than x characters\n",
    "data = data.loc[data['flines'].str.len() > 5]\n",
    "# Remove users with entries < 25\n",
    "data[\"username\"].value_counts()\n",
    "data = data[data['username'].map(data['username'].value_counts()) > 25].reset_index(drop = True)\n",
    "\n",
    "#data = data[:1000]\n",
    "#print(data[\"flines\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AKTechie      63\n",
       "Jimb          52\n",
       "Qingchun      47\n",
       "amihk         46\n",
       "bmerry        45\n",
       "              ..\n",
       "XiaoZiqian    26\n",
       "domeng        26\n",
       "elizarov      26\n",
       "stone         26\n",
       "kitamasa      26\n",
       "Name: username, Length: 122, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"username\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3892 entries, 0 to 3891\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  3892 non-null   int64 \n",
      " 1   year        3892 non-null   int64 \n",
      " 2   round       3892 non-null   int64 \n",
      " 3   username    3892 non-null   object\n",
      " 4   task        3892 non-null   int64 \n",
      " 5   solution    3892 non-null   int64 \n",
      " 6   file        3892 non-null   object\n",
      " 7   full_path   3892 non-null   object\n",
      " 8   flines      3892 non-null   object\n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 273.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n // Headers {{{\\n #include<iostream>\\n #include<assert.h>\\n #include<cstdio>\\n #include<cctype>\\n #include<cmath>\\n #include<cstdlib>\\n #include<algorithm>\\n #include<vector>\\n #include<string>\\n #include<list>\\n #include<deque>\\n #include<map>\\n #include<set>\\n #include<queue>\\n #include<stack>\\n #include<utility>\\n #include<sstream>\\n #include<cstring>\\n #include<bitset>\\n #include<numeric>\\n using namespace std;\\n \\n \\n #define FOR(I,A,B) for(int I=(A);I<=(B);++I)\\n #define FORD(I,A,B) for(int I=(A);I>=(B);--I)\\n #define REP(I,N) for(int I=0;I<(N);++I)\\n #define VAR(V,init) __typeof(init) V=(init)\\n #define FORE(I,C) for(VAR(I,(C).begin());I!=(C).end();++I)\\n #define CLR(A,v) memset((A),v,sizeof((A)))\\n \\n #define SIZE(x) ((int)((x).size()))\\n #define ALL(X) (X).begin(),(X).end()\\n #define PB push_back\\n #define MP make_pair\\n #define FI first\\n #define SE second\\n \\n typedef vector<int> VI;\\n typedef pair<int,int> PI;\\n typedef long long LL;\\n typedef vector<string> VS;\\n // }}}\\n \\n \\n void error(string s)\\n {\\n \\tfprintf(stderr,\"%s\\\\n\",s.c_str());\\n \\texit(1);\\n }\\n \\n LL mod=10007;\\n LL T[500][500];\\n bool B[500][500];\\n int tx,ty;\\n LL fun(int y,int x)\\n {\\n \\tif (tx==x && ty==y) return 1;\\n \\tif (y>ty || x>tx) return 0;\\n \\tif (B[y][x]) return 0;\\n \\tif (T[y][x]!=-1) return T[y][x];\\n \\tT[y][x]=(fun(y+2,x+1)+fun(y+1,x+2))%mod;\\n \\treturn T[y][x];\\n }\\n \\n int main(int argc,char **args)\\n {\\n \\tif (argc != 3) error(\"Bad arguments\");\\n \\tFILE *in=fopen(args[1],\"r\");\\n \\tFILE *out=fopen(args[2],\"w\");\\n \\tint z; fscanf(in,\"%d\",&z);\\n \\tREP(zz,z)\\n \\t{\\n \\t\\tfprintf(stderr,\"Working on %d / %d\\\\n\",zz+1,z);\\n \\t\\tCLR(B,0);\\n \\t\\tCLR(T,-1);\\n \\t\\tint w,h,r;\\n \\t\\tfscanf(in,\"%d%d%d\",&tx,&ty,&r);\\n \\t\\twhile(r--)\\n \\t\\t{\\n \\t\\t\\tfscanf(in,\"%d%d\",&w,&h);\\n \\t\\t\\tB[h][w]=1;\\n \\t\\t}\\n \\n \\n \\t\\tfprintf(out,\"Case #%d: %lld\\\\n\",zz+1,fun(1,1));\\n \\t}\\n \\tfclose(in); fclose(out);\\n \\treturn 0;\\n }\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.flines[20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Functions for data preprocessing/ feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "def create_metrics(code):\n",
    "        \"\"\"\n",
    "        Creates metrics needed for modeling\n",
    "        :return: returns an Array with metrics calculated from the source code\n",
    "        \"\"\"\n",
    "        # TODO: assign vars for redundant code\n",
    "        # TODO: self.var for all variable assignment\n",
    "\n",
    "        # Number of characters in code/ length of code\n",
    "        code_len = len(code)\n",
    "\n",
    "        # Number of characters in line\n",
    "        code_tokens = code.split(\"\\n\")\n",
    "        nchar_in_line = mean([code_len for token in code_tokens])\n",
    "\n",
    "        # Lines of code / file length in chars\n",
    "        n_lines = code.count('\\n')\n",
    "\n",
    "        # Create n-grams\n",
    "        #unigram = word2ngrams(text = self.code, n = 1)\n",
    "        #bigram = word2ngrams(text = self.code, n = 2)\n",
    "        #trigram = word2ngrams(text = self.code, n = 3)\n",
    "\n",
    "        # Number of words in the text / file length in characters\n",
    "        word_len_ratio = len(code.split())/code_len\n",
    "\n",
    "        # Average number of words per line / file length in characters\n",
    "        pre = (code.split('\\n'))\n",
    "        words_list = [x.split() for i, x in enumerate(pre)]\n",
    "        words_count = [len(words_list[i]) for i, x in enumerate(pre)]\n",
    "        avg_words_per_line = np.mean(words_count)\n",
    "\n",
    "        # Number of whitespace / file length in characters\n",
    "        whitespace_ratio = code.count(' ')/code_len\n",
    "\n",
    "        # Number of line breaks / file length in characters\n",
    "        linebreak_ratio = code.count('\\n')/len(code)\n",
    "\n",
    "        # Number of indentations / file length in characters\n",
    "        indent_ratio = code.count('\\t')/code_len\n",
    "\n",
    "        # Number of upper_case words / file length in characters\n",
    "        uppercase_ratio = sum(1 for char in code if char.isupper())/code_len\n",
    "\n",
    "        # Number of lower_case words / file length in characters\n",
    "        lowercase_ratio = sum(1 for char in code if char.islower())/code_len\n",
    "\n",
    "        # Number of punctuation symbols / file length in characters\n",
    "        punctuation_count = Counter(punc for line in code for punc in line if punc in punctuation)\n",
    "        punctuation_count_ratio = sum(punctuation_count.values())/ code_len\n",
    "\n",
    "        # Average length of words(exluding punctuation, excluding single letter or number)\n",
    "        #code_no_punc = code\n",
    "        #for punc in punctuation:\n",
    "        #    code_no_punc = code_no_punc.replace(punc, '')\n",
    "        #code_no_single_char = ' '.join([w for w in code_no_punc.split() if len(w)>1])\n",
    "        #list_len_words = [len(x) for i, x in enumerate(code_no_single_char.split())]\n",
    "        #avg_char_per_word = np.mean(list_len_words)\n",
    "\n",
    "        #Keywords count ('if', 'else', 'while', 'for', 'in', 'elif', 'or', 'not', 'with', 'and', 'is')\n",
    "        keyword_count_ratio = sum([code.count(x) for x in ['if', 'else', 'while', 'for', 'in', 'elif', 'or', 'not', 'with', 'and', 'is'] ]) / code_len\n",
    "\n",
    "        # Return all metrics as array\n",
    "        return pd.Series([code_len, nchar_in_line, n_lines, word_len_ratio, avg_words_per_line, whitespace_ratio,linebreak_ratio,indent_ratio,uppercase_ratio,lowercase_ratio, punctuation_count_ratio,keyword_count_ratio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_len</th>\n",
       "      <th>nchar_in_line</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>word_len_ratio</th>\n",
       "      <th>avg_words_per_line</th>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <th>linebreak_ratio</th>\n",
       "      <th>indent_ratio</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>lowercase_ratio</th>\n",
       "      <th>punctuation_count_ratio</th>\n",
       "      <th>keyword_count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1143.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.196850</td>\n",
       "      <td>4.787234</td>\n",
       "      <td>0.359580</td>\n",
       "      <td>0.040245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017498</td>\n",
       "      <td>0.336833</td>\n",
       "      <td>0.219598</td>\n",
       "      <td>0.036745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3300.0</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.187273</td>\n",
       "      <td>5.024390</td>\n",
       "      <td>0.374242</td>\n",
       "      <td>0.036970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024848</td>\n",
       "      <td>0.327879</td>\n",
       "      <td>0.211818</td>\n",
       "      <td>0.025455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3300.0</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.187273</td>\n",
       "      <td>5.024390</td>\n",
       "      <td>0.374242</td>\n",
       "      <td>0.036970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024848</td>\n",
       "      <td>0.327879</td>\n",
       "      <td>0.211818</td>\n",
       "      <td>0.025455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3022.0</td>\n",
       "      <td>3022.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.178028</td>\n",
       "      <td>4.236220</td>\n",
       "      <td>0.447717</td>\n",
       "      <td>0.041694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.272336</td>\n",
       "      <td>0.198544</td>\n",
       "      <td>0.030774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1658.0</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.193607</td>\n",
       "      <td>5.095238</td>\n",
       "      <td>0.415561</td>\n",
       "      <td>0.037394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016285</td>\n",
       "      <td>0.282871</td>\n",
       "      <td>0.223160</td>\n",
       "      <td>0.037394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>1926.0</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>2.609756</td>\n",
       "      <td>0.361371</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021807</td>\n",
       "      <td>0.397196</td>\n",
       "      <td>0.154725</td>\n",
       "      <td>0.032710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3888</th>\n",
       "      <td>797.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.119197</td>\n",
       "      <td>2.878788</td>\n",
       "      <td>0.343789</td>\n",
       "      <td>0.040151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>0.377666</td>\n",
       "      <td>0.181932</td>\n",
       "      <td>0.046424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>797.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.119197</td>\n",
       "      <td>2.878788</td>\n",
       "      <td>0.343789</td>\n",
       "      <td>0.040151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>0.377666</td>\n",
       "      <td>0.181932</td>\n",
       "      <td>0.046424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>733.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.117326</td>\n",
       "      <td>2.529412</td>\n",
       "      <td>0.291951</td>\n",
       "      <td>0.045020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.482947</td>\n",
       "      <td>0.160982</td>\n",
       "      <td>0.047749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>733.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.117326</td>\n",
       "      <td>2.529412</td>\n",
       "      <td>0.291951</td>\n",
       "      <td>0.045020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.482947</td>\n",
       "      <td>0.160982</td>\n",
       "      <td>0.047749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3892 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      code_len  nchar_in_line  n_lines  word_len_ratio  avg_words_per_line  \\\n",
       "0       1143.0         1143.0     46.0        0.196850            4.787234   \n",
       "1       3300.0         3300.0    122.0        0.187273            5.024390   \n",
       "2       3300.0         3300.0    122.0        0.187273            5.024390   \n",
       "3       3022.0         3022.0    126.0        0.178028            4.236220   \n",
       "4       1658.0         1658.0     62.0        0.193607            5.095238   \n",
       "...        ...            ...      ...             ...                 ...   \n",
       "3887    1926.0         1926.0     81.0        0.111111            2.609756   \n",
       "3888     797.0          797.0     32.0        0.119197            2.878788   \n",
       "3889     797.0          797.0     32.0        0.119197            2.878788   \n",
       "3890     733.0          733.0     33.0        0.117326            2.529412   \n",
       "3891     733.0          733.0     33.0        0.117326            2.529412   \n",
       "\n",
       "      whitespace_ratio  linebreak_ratio  indent_ratio  uppercase_ratio  \\\n",
       "0             0.359580         0.040245           0.0         0.017498   \n",
       "1             0.374242         0.036970           0.0         0.024848   \n",
       "2             0.374242         0.036970           0.0         0.024848   \n",
       "3             0.447717         0.041694           0.0         0.007280   \n",
       "4             0.415561         0.037394           0.0         0.016285   \n",
       "...                ...              ...           ...              ...   \n",
       "3887          0.361371         0.042056           0.0         0.021807   \n",
       "3888          0.343789         0.040151           0.0         0.010038   \n",
       "3889          0.343789         0.040151           0.0         0.010038   \n",
       "3890          0.291951         0.045020           0.0         0.004093   \n",
       "3891          0.291951         0.045020           0.0         0.004093   \n",
       "\n",
       "      lowercase_ratio  punctuation_count_ratio  keyword_count_ratio  \n",
       "0            0.336833                 0.219598             0.036745  \n",
       "1            0.327879                 0.211818             0.025455  \n",
       "2            0.327879                 0.211818             0.025455  \n",
       "3            0.272336                 0.198544             0.030774  \n",
       "4            0.282871                 0.223160             0.037394  \n",
       "...               ...                      ...                  ...  \n",
       "3887         0.397196                 0.154725             0.032710  \n",
       "3888         0.377666                 0.181932             0.046424  \n",
       "3889         0.377666                 0.181932             0.046424  \n",
       "3890         0.482947                 0.160982             0.047749  \n",
       "3891         0.482947                 0.160982             0.047749  \n",
       "\n",
       "[3892 rows x 12 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#metrics = data_small.flines.apply(lambda x:create_metrics(code = x))\n",
    "metrics = pd.DataFrame()\n",
    "metrics[[\"code_len\", \"nchar_in_line\", \"n_lines\", \"word_len_ratio\", \"avg_words_per_line\", \"whitespace_ratio\",\"linebreak_ratio\",\"indent_ratio\",\"uppercase_ratio\",\"lowercase_ratio\", \"punctuation_count_ratio\",\"keyword_count_ratio\"]] = data.flines.apply(lambda x:create_metrics(code = x))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling only with metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Label Encoder has encoded the classes into ['ACRush' 'AKTechie' 'AS1' 'Ahyangyi' 'Alexus' 'Amber' 'Astein' 'Bohua'\n",
      " 'Chmel.Tolstiy' 'DNNX' 'DmitryKlenov' 'Eryx' 'Fire' 'Gluk' 'Hammer'\n",
      " 'HiltonLange' 'Huayang' 'Innovative.Cat' 'Jacek' 'JanKuipers' 'Jedi'\n",
      " 'Jimb' 'JongMan' 'KOTEHOK' 'KUNES' 'Klinck' 'LayCurse' 'LinesPrower'\n",
      " 'Loner' 'LucaB' 'Lunarmony' 'MB.' 'OpenGL' 'PaulJefferys' 'Qingchun'\n",
      " 'Reid' 'Robinnibor' 'Savior' 'SkidanovAlexander' 'Soultaker' 'TripleM'\n",
      " 'Vasyl' 'Vedensky' 'Vitaliy' 'Vytis' 'XiaoZiqian' 'Yarin' 'Ying' 'almelv'\n",
      " 'amihk' 'andersk' 'andrewzta' 'antimatter' 'ardiankp' 'austrin'\n",
      " 'blueblimp' 'bmerry' 'burunduk3' 'darnley' 'darthur' 'dgozman' 'domeng'\n",
      " 'dzhulgakov' 'dzwiedziu' 'eagleonhill' 'elizarov' 'falagar' 'ftc'\n",
      " 'fuwenjie' 'g201513' 'gawry' 'guitarboy' 'gusakov' 'halyavin' 'henshiru'\n",
      " 'hmich' 'humblefool' 'ilyakor' 'ilyaraz' 'ivan.popelyshev' 'iwi' 'jakubr'\n",
      " 'jpsbur' 'kinaba' 'kitamasa' 'klopyrev' 'kp7' 'krijgertje' 'ltdtl'\n",
      " 'lympanda' 'macs' 'misof' 'moranlf' 'mystic' 'natalia' 'nik239' 'nika'\n",
      " 'nya' 'oberon' 'pashka' 'pdallago' 'ploh' 'pmnox' 'radeye' 'ralph.maroun'\n",
      " 'slex' 'ssancho' 'stone' 'tanakh' 'tckwok' 'tos.lunar' 'tourist' 'updog'\n",
      " 'vlad89' 'wata' 'windy7926778' 'winger' 'xreborner' 'yiuyuho' 'ymatsux'\n",
      " 'yuhch123' 'zibada']\n"
     ]
    }
   ],
   "source": [
    "# Encode Label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit it to the target\n",
    "label_encoder.fit(data[\"username\"])\n",
    "\n",
    "# Find the encoded classes\n",
    "print(f\"The Label Encoder has encoded the classes into {label_encoder.classes_}\")\n",
    "\n",
    "# Transform the targets\n",
    "data[\"username_encoded\"] = label_encoder.transform(data[\"username\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = metrics\n",
    "y = data[\"username_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AKTechie      63\n",
       "Jimb          52\n",
       "Qingchun      47\n",
       "amihk         46\n",
       "bmerry        45\n",
       "              ..\n",
       "XiaoZiqian    26\n",
       "domeng        26\n",
       "elizarov      26\n",
       "stone         26\n",
       "kitamasa      26\n",
       "Name: username, Length: 122, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"username\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code_len                   0\n",
       "nchar_in_line              0\n",
       "n_lines                    0\n",
       "word_len_ratio             0\n",
       "avg_words_per_line         0\n",
       "whitespace_ratio           0\n",
       "linebreak_ratio            0\n",
       "indent_ratio               0\n",
       "uppercase_ratio            0\n",
       "lowercase_ratio            0\n",
       "punctuation_count_ratio    0\n",
       "keyword_count_ratio        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07445443 0.09499358 0.08868895 0.08868895 0.07712082]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MultinomialNB()\n",
    "print(cross_val_score(clf, X, y, cv=5))\n",
    "#clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=1000, verbosity=2, scoring='recall', n_jobs=-1, cv=5)\n",
    "\n",
    "# Process autoML with TPOT\n",
    "tpot.fit(X, y)\n",
    "\n",
    "# Print score\n",
    "print(tpot.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 13)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#metrics = metrics.to_numpy()\n",
    "metrics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create Count Vectorization for text + ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(flines, analyzer = 'word', ngram_range = (1, 1), min_df = 3):\n",
    "    \"\"\"\n",
    "    Apply count vectorizer to a given column (flines)\n",
    "    :param analyzer:\n",
    "        if 'word': vectorization happens with word n-grams\n",
    "        if 'char': vectorization happens with character n-grams, padding the empty space to the tokens\n",
    "        if 'char_wb': creates character n-grams only from text inside word boundaries\n",
    "    :param ngram_range: the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.\n",
    "        examples: (1, 1) extracts unigrams, (1, 2) extracts unigrams and bigrams\n",
    "    :param min_df: remove terms with frequency lower than threshold\n",
    "    :return: array of count_frequencies\n",
    "    \"\"\"\n",
    "    count_vectorizer = CountVectorizer(analyzer = analyzer, ngram_range = ngram_range)\n",
    "    X = count_vectorizer.fit_transform(flines).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply count_vectorizer to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vec = count_vectorizer(flines = data_small[\"flines\"])\n",
    "word_count_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply count_vectorizer to n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [11,  0,  0, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_count_vec = count_vectorizer(flines = data_small[\"flines\"], analyzer = \"char\", ngram_range = (1, 3))\n",
    "ngram_count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3892, 12), (1000, 6598), (1000, 35619))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.shape, word_count_vec.shape, ngram_count_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3892 and the array at index 1 has size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#np.append([metrics, word_count_vec, ngram_count_vec], values = 1)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate( (metrics, word_count_vec, ngram_count_vec), axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3892 and the array at index 1 has size 1000"
     ]
    }
   ],
   "source": [
    "#np.append([metrics, word_count_vec, ngram_count_vec], values = 1)\n",
    "\n",
    "res = np.concatenate( (metrics, word_count_vec, ngram_count_vec), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "y = data[\"username_encoded\"]\n",
    "X = word_count_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1000, 3892]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m clf \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(cross_val_score(clf, X, y, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m))\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:252\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcross_validate\u001b[39m(\n\u001b[1;32m     50\u001b[0m     estimator,\n\u001b[1;32m     51\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     error_score\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan,\n\u001b[1;32m     64\u001b[0m ):\n\u001b[1;32m     65\u001b[0m     \u001b[39m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    254\u001b[0m     cv \u001b[39m=\u001b[39m check_cv(cv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/utils/validation.py:433\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 433\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/utils/validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 3892]"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "print(cross_val_score(clf, X, y, cv=5))\n",
    "#clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values in feature set\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Input data is not in a valid format. Please confirm that the input data is scikit-learn compatible. For example, the features must be a 2-D array and target labels must be a 1-D array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:1380\u001b[0m, in \u001b[0;36mTPOTBase._check_dataset\u001b[0;34m(self, features, target, sample_weight)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1380\u001b[0m     X, y \u001b[39m=\u001b[39m check_X_y(features, target, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1381\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_imputed:\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/utils/validation.py:1092\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> 1092\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1094\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/utils/validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 3923]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [198], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m tpot \u001b[39m=\u001b[39m TPOTClassifier(generations\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, population_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, verbosity\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Process autoML with TPOT\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tpot\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Print score\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(tpot\u001b[39m.\u001b[39mscore(X, y))\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:726\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39m\"\"\"Fit an optimized machine learning pipeline.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \n\u001b[1;32m    690\u001b[0m \u001b[39mUses genetic programming to optimize a machine learning pipeline that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    723\u001b[0m \n\u001b[1;32m    724\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_init()\n\u001b[0;32m--> 726\u001b[0m features, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_dataset(features, target, sample_weight)\n\u001b[1;32m    728\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_pretest(features, target)\n\u001b[1;32m    730\u001b[0m \u001b[39m# Randomly collect a subsample of training samples for pipeline optimization process.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:1392\u001b[0m, in \u001b[0;36mTPOTBase._check_dataset\u001b[0;34m(self, features, target, sample_weight)\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[39mreturn\u001b[39;00m features\n\u001b[1;32m   1391\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAssertionError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m-> 1392\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1393\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError: Input data is not in a valid format. Please confirm \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthat the input data is scikit-learn compatible. For example, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe features must be a 2-D array and target labels must be a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m1-D array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1397\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Input data is not in a valid format. Please confirm that the input data is scikit-learn compatible. For example, the features must be a 2-D array and target labels must be a 1-D array."
     ]
    }
   ],
   "source": [
    "# Instantiate TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=4, population_size=20, verbosity=2, scoring='recall', n_jobs=-1, cv=5)\n",
    "\n",
    "# Process autoML with TPOT\n",
    "tpot.fit(X, y)\n",
    "\n",
    "# Print score\n",
    "print(tpot.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate T-Pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values in feature set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362cfda9634f40aab584df541f290d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/timcerta/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -inf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_splits=5 cannot be greater than the number of members in each class.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/joblib/parallel.py:825\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    826\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[1;32m    827\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    832\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/queue.py:167\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    168\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:816\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    815\u001b[0m         warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 816\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pop, _ \u001b[39m=\u001b[39m eaMuPlusLambda(\n\u001b[1;32m    817\u001b[0m             population\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pop,\n\u001b[1;32m    818\u001b[0m             toolbox\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_toolbox,\n\u001b[1;32m    819\u001b[0m             mu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpopulation_size,\n\u001b[1;32m    820\u001b[0m             lambda_\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lambda,\n\u001b[1;32m    821\u001b[0m             cxpb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrossover_rate,\n\u001b[1;32m    822\u001b[0m             mutpb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutation_rate,\n\u001b[1;32m    823\u001b[0m             ngen\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerations,\n\u001b[1;32m    824\u001b[0m             pbar\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pbar,\n\u001b[1;32m    825\u001b[0m             halloffame\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pareto_front,\n\u001b[1;32m    826\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbosity,\n\u001b[1;32m    827\u001b[0m             per_generation_function\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_periodic_pipeline,\n\u001b[1;32m    828\u001b[0m             log_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_file_,\n\u001b[1;32m    829\u001b[0m         )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Allow for certain exceptions to signal a premature fit() cancellation\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/gp_deap.py:281\u001b[0m, in \u001b[0;36meaMuPlusLambda\u001b[0;34m(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function, log_file)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m per_generation_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     per_generation_function(gen)\n\u001b[1;32m    283\u001b[0m \u001b[39m# Update the statistics with the new population\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:1176\u001b[0m, in \u001b[0;36mTPOTBase._check_periodic_pipeline\u001b[0;34m(self, gen)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[39m\"\"\"If enough time has passed, save a new optimized pipeline. Currently used in the per generation hook in the optimization loop.\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[39m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_top_pipeline()\n\u001b[1;32m   1177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperiodic_checkpoint_folder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:924\u001b[0m, in \u001b[0;36mTPOTBase._update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_val_score\n\u001b[0;32m--> 924\u001b[0m cv_scores \u001b[39m=\u001b[39m cross_val_score(\n\u001b[1;32m    925\u001b[0m     sklearn_pipeline,\n\u001b[1;32m    926\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretest_X,\n\u001b[1;32m    927\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretest_y,\n\u001b[1;32m    928\u001b[0m     cv\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv,\n\u001b[1;32m    929\u001b[0m     scoring\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring_function,\n\u001b[1;32m    930\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    931\u001b[0m     error_score\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mraise\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    933\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/joblib/parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/joblib/parallel.py:836\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    834\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[0;32m--> 836\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:340\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m         (\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot have number of splits n_splits=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m greater\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m than the number of samples: n_samples=\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m         )\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    338\u001b[0m     )\n\u001b[0;32m--> 340\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    341\u001b[0m     \u001b[39myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:86\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     85\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(_num_samples(X))\n\u001b[0;32m---> 86\u001b[0m \u001b[39mfor\u001b[39;00m test_index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_test_masks(X, y, groups):\n\u001b[1;32m     87\u001b[0m     train_index \u001b[39m=\u001b[39m indices[np\u001b[39m.\u001b[39mlogical_not(test_index)]\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:717\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iter_test_masks\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 717\u001b[0m     test_folds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_test_folds(X, y)\n\u001b[1;32m    718\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits):\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:679\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mall(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m y_counts):\n\u001b[0;32m--> 679\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    680\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_splits=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m cannot be greater than the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m number of members in each class.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits)\n\u001b[1;32m    682\u001b[0m     )\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m min_groups:\n",
      "\u001b[0;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/joblib/parallel.py:825\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     tasks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ready_batches\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    826\u001b[0m \u001b[39mexcept\u001b[39;00m queue\u001b[39m.\u001b[39mEmpty:\n\u001b[1;32m    827\u001b[0m     \u001b[39m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     \u001b[39m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[39m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    832\u001b[0m     \u001b[39m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/queue.py:167\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    168\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [176], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m tpot \u001b[39m=\u001b[39m TPOTClassifier(generations\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, population_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, verbosity\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Process autoML with TPOT\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m tpot\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Print score\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(tpot\u001b[39m.\u001b[39mscore(X, y))\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:863\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mSystemExit\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    861\u001b[0m         \u001b[39m# raise the exception if it's our last attempt\u001b[39;00m\n\u001b[1;32m    862\u001b[0m         \u001b[39mif\u001b[39;00m attempt \u001b[39m==\u001b[39m (attempts \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 863\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    864\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:854\u001b[0m, in \u001b[0;36mTPOTBase.fit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar, \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m)):\n\u001b[1;32m    852\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pbar\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 854\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_top_pipeline()\n\u001b[1;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_of_best_pipeline(features, target)\n\u001b[1;32m    856\u001b[0m \u001b[39m# Delete the temporary cache before exiting\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/tpot/base.py:924\u001b[0m, in \u001b[0;36mTPOTBase._update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    921\u001b[0m         sklearn_pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_toolbox\u001b[39m.\u001b[39mcompile(expr\u001b[39m=\u001b[39mpipeline)\n\u001b[1;32m    922\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_val_score\n\u001b[0;32m--> 924\u001b[0m         cv_scores \u001b[39m=\u001b[39m cross_val_score(\n\u001b[1;32m    925\u001b[0m             sklearn_pipeline,\n\u001b[1;32m    926\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretest_X,\n\u001b[1;32m    927\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretest_y,\n\u001b[1;32m    928\u001b[0m             cv\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv,\n\u001b[1;32m    929\u001b[0m             scoring\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring_function,\n\u001b[1;32m    930\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    931\u001b[0m             error_score\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mraise\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    932\u001b[0m         )\n\u001b[1;32m    933\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    935\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThere was an error in the TPOT optimization \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprocess. This could be because the data was \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttps://epistasislab.github.io/tpot/using/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    945\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/joblib/parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1049\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/joblib/parallel.py:836\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    834\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[0;32m--> 836\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:340\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m n_samples:\n\u001b[1;32m    333\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m         (\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot have number of splits n_splits=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m greater\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m than the number of samples: n_samples=\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m         )\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    338\u001b[0m     )\n\u001b[0;32m--> 340\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    341\u001b[0m     \u001b[39myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:86\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     84\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m     85\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(_num_samples(X))\n\u001b[0;32m---> 86\u001b[0m \u001b[39mfor\u001b[39;00m test_index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_test_masks(X, y, groups):\n\u001b[1;32m     87\u001b[0m     train_index \u001b[39m=\u001b[39m indices[np\u001b[39m.\u001b[39mlogical_not(test_index)]\n\u001b[1;32m     88\u001b[0m     test_index \u001b[39m=\u001b[39m indices[test_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:717\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iter_test_masks\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 717\u001b[0m     test_folds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_test_folds(X, y)\n\u001b[1;32m    718\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits):\n\u001b[1;32m    719\u001b[0m         \u001b[39myield\u001b[39;00m test_folds \u001b[39m==\u001b[39m i\n",
      "File \u001b[0;32m~/.pyenv/versions/xref/lib/python3.8/site-packages/sklearn/model_selection/_split.py:679\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    677\u001b[0m min_groups \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmin(y_counts)\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mall(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m y_counts):\n\u001b[0;32m--> 679\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    680\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_splits=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m cannot be greater than the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m number of members in each class.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits)\n\u001b[1;32m    682\u001b[0m     )\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m min_groups:\n\u001b[1;32m    684\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    685\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe least populated class in y has only \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m members, which is less than n_splits=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    687\u001b[0m         \u001b[39m%\u001b[39m (min_groups, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits),\n\u001b[1;32m    688\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m    689\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: n_splits=5 cannot be greater than the number of members in each class."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "X = df2\n",
    "y = data_small[\"username\"]\n",
    "\n",
    "# Instantiate TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=4, population_size=20, verbosity=2, scoring='recall', n_jobs=-1, cv=5)\n",
    "\n",
    "# Process autoML with TPOT\n",
    "tpot.fit(X, y)\n",
    "\n",
    "# Print score\n",
    "print(tpot.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('xref')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5de49888c34cc98682ba707a14312ef22222df4f8946f096549f7af996c677f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
