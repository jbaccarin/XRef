{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Downloading files from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(start=2008, end=2020):\n",
    "    \"\"\"\n",
    "    Downloads all .tar files from github (https://github.com/Jur1cek/gcj-dataset/blob/master/)\n",
    "    Unzips it to a directory as the original .csv files\n",
    "    \"\"\"\n",
    "    \n",
    "    years = np.arange(start, end+1)\n",
    "    \n",
    "    for year in years:\n",
    "        dirpath = os.path.dirname(os.getcwd())\n",
    "        csvpath = f'raw_data/gcj{year}.csv'\n",
    "        untarpath = f'raw_data'\n",
    "        \n",
    "        fullcsvpath = os.path.join(dirpath,csvpath)\n",
    "        fulluntarpath = os.path.join(dirpath,untarpath)\n",
    "        \n",
    "        if os.path.isfile(fullcsvpath):\n",
    "            pass\n",
    "        else:\n",
    "            urlf = urlopen(f\"https://github.com/Jur1cek/gcj-dataset/raw/master/gcj{year}.csv.tar.bz2\")\n",
    "            tarf = tarfile.open(name=None, fileobj=BytesIO(urlf.read()))\n",
    "            tarf.extractall(fulluntarpath)\n",
    "            tarf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Compressing and Cleaning data \n",
    "### Getting the last code submission from each developer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, **kwargs):\n",
    "    \"\"\"Removing older code submissions, leaving only the last one available\n",
    "    Lower index values correspond to more recent solutions\"\"\"\n",
    "    \n",
    "    # picking and reordering columns for each data frame\n",
    "    df = df[['year','round','username','task','file','flines','full_path']]\n",
    "    \n",
    "    # dropping code submissions that are not the latest ones\n",
    "    df = df.drop_duplicates(subset=['year', 'round', 'username', 'task'], keep='first')\n",
    "    df = df.dropna()\n",
    "       \n",
    "    # forcing string conversion\n",
    "    df['task'] = df['task'].str.lstrip(\"0\")\n",
    "    df['file'] = df['file'].str.lstrip(\"0\").str.lower()\n",
    "    df['full_path'] = df['full_path'].str.lower()\n",
    "    df['round'] = df['round'].str.lstrip(\"0\")    \n",
    "\n",
    "\n",
    "    # finding the language of code\n",
    "    df.loc[df['year'] == 2020, 'code_lang'] = df['full_path']\n",
    "    df.loc[df['year'] != 2020, 'code_lang'] = df['file']\n",
    "    df['code_lang'] = df['code_lang'].str.split(\".\").str[-1]\n",
    "    \n",
    "    # fixing python3 definition\n",
    "    df.loc[df['code_lang'] == 'python3', 'code_lang'] = \"py\"\n",
    "    \n",
    "    # getting the length of source code\n",
    "    df['code_len'] = df['flines'].str.len()\n",
    "\n",
    "    # selecting before appending to csv file      \n",
    "    df = df.rename(columns={'file':'file_name','flines':'code_source'})\n",
    "    df = df.drop(columns=['full_path'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files():\n",
    "    tar_path = os.path.join(os.path.dirname(os.getcwd()),'raw_data')\n",
    "    raw_files = os.listdir(tar_path)\n",
    "    cleaned_path = os.path.join(os.path.dirname(os.getcwd()),'raw_data/cleaned_dataset.csv')\n",
    "\n",
    "    # dff = pd.DataFrame(columns=['year','round','username','task','file_name','code_source','code_lang', 'code_len'])\n",
    "\n",
    "    if os.path.isfile(cleaned_path):\n",
    "        os.remove(cleaned_path)\n",
    "        print(\"File already exists - deleting old version\")\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    for csv_file in raw_files:\n",
    "        if csv_file.endswith('.csv'):\n",
    "            fullcsvpath = os.path.join(tar_path, csv_file)\n",
    "            \n",
    "            # specifying which columns to use here is more efficient than having to drop them afterwards\n",
    "            df = pd.read_csv(fullcsvpath,\n",
    "                            low_memory=False, \n",
    "                            usecols=['year','round','username','task','file','flines','full_path'],\n",
    "                            encoding='utf-8',\n",
    "                            dtype={'year':'int16',\n",
    "                                    'round':'str',\n",
    "                                    'username':'str',\n",
    "                                    'task':'str',\n",
    "                                    'file':'str',\n",
    "                                    'flines':'str',\n",
    "                                    'full_path':'str'}) \n",
    "            df = clean_data(df)\n",
    "            column_names = df.columns\n",
    "            df = df[['year','round','username','task','file_name','code_source','code_lang','code_len']]\n",
    "    \n",
    "            if not os.path.isfile(cleaned_path):\n",
    "                df.to_csv(cleaned_path, header=column_names, index=False)\n",
    "            else:  # else it exists so append without writing the header\n",
    "                df.to_csv(cleaned_path, mode='a', header=False, index=False)\n",
    "    return print(f\"New file created at {cleaned_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Preprocessing\n",
    "- Removing programming languages that are not relevant [java, py, cpp]\n",
    "- Removing code submissions that are too short, too lenghty (500<x<25000 characters)\n",
    "- Removing developers/coders who did not participate in many challenges (>5 rounds per year)\n",
    "- Removing code submissions that are too alike for the same author and same round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1) Top Languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(936231, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_path = os.path.join(os.path.dirname(os.getcwd()),'raw_data/cleaned_dataset.csv')\n",
    "dc = pd.read_csv(cleaned_path, \n",
    "                             low_memory=False, \n",
    "                             encoding='utf-8')\n",
    "\n",
    "dc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = dc['code_lang'].value_counts(normalize=True).to_frame()\n",
    "vc['pareto'] = vc['code_lang'].cumsum()\n",
    "vc.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2) Ideal code length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.code_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.set()\n",
    "\n",
    "sns.histplot(x=dc.code_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.set()\n",
    "dcl = dc[dc['code_len'] < 4000]\n",
    "\n",
    "sns.boxplot(x=dcl.code_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounds per year\n",
    "dc.groupby('year', as_index=False).agg({'round':'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenges per year\n",
    "dc.groupby('year', as_index=False).agg({'task':'nunique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenges per year\n",
    "top_authors = dc.groupby(['username', 'year'], as_index=False).agg({'round':'nunique', 'task': 'nunique'})\n",
    "top_authors = top_authors.sort_values('round', ascending=False)\n",
    "\n",
    "top_authors = top_authors[(top_authors['round']>=5) & (top_authors['task']>=12)]\n",
    "\n",
    "top_authors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=top_authors['round'], x='round')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most dedicated developers have code_len between 500(min) and 15000(max)\n",
    "top_authors = dc.groupby(['username', 'year'], as_index=False).agg({'round':'nunique', 'task': 'nunique', 'code_len':'min'})\n",
    "top_authors = top_authors.sort_values('round', ascending=False)\n",
    "top_authors = top_authors[(top_authors['round']>=5) & (top_authors['task']>=12)]\n",
    "\n",
    "top_authors.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def features_preprocessing(df):\n",
    "\n",
    "    # picking the most relevant coding languages\n",
    "    languages = ['cpp', 'py', 'java']\n",
    "    df = df[df['code_lang'].isin(languages)]\n",
    "    \n",
    "    # removing too short, too lenghty code - based on most frequent coders\n",
    "    df = df.query('code_len > 500 and code_len < 15000')\n",
    "    \n",
    "    # keeping only developers who participated in more than 4 rounds and 50% fo the competition\n",
    "    top_authors = df.groupby(['username', 'year'], as_index=False).agg({'round':'nunique', 'task': 'nunique'})\n",
    "    top_authors = top_authors.sort_values('round', ascending=False)\n",
    "    top_authors = top_authors[(top_authors['round']>=5) & (top_authors['task']>=12)]\n",
    "    ta_list = top_authors['username']\n",
    "    df = df[df['username'].isin(ta_list)]\n",
    "    \n",
    "    # sorting columns and rows\n",
    "    df = df[['username', 'year', 'round','task', 'code_len', 'code_lang', 'code_source']]\n",
    "    df = df.sort_values(['username','year', 'round','code_len'], ascending=True)\n",
    "\n",
    "    # shifting rows down\n",
    "    df['next_username'] = df['username'].shift(-1)\n",
    "    df['next_year'] = df['year'].shift(-1)\n",
    "    df['next_round'] = df['round'].shift(-1)\n",
    "    df['next_code'] = df['code_source'].shift(-1)\n",
    "    df['next_code_len'] = df['code_len'].shift(-1)\n",
    "    \n",
    "    # filling Na values after shift\n",
    "    df['next_code_len'] = df['next_code_len'].fillna(0)\n",
    "    df['next_year'] = df['next_year'].fillna(0)\n",
    "    \n",
    "    # fixing fields dtypes\n",
    "    df['next_year'] = df['year'].astype(int)\n",
    "    df['next_code_len'] = df['next_code_len'].astype(int)\n",
    "    df['next_code'] = df['next_code'].astype(str)\n",
    "    df['code_len'] = df['code_len'].astype(int)    \n",
    "    \n",
    "    # # cutting original code by the legth of previous code\n",
    "    df['next_code_cut'] = df.apply(lambda x: x['next_code'][0:x['code_len']] ,axis=1)    \n",
    "\n",
    "    #calculating distance between strings\n",
    "    df['string_distance'] = df.apply(lambda x: ratio(x['next_code_cut'], x['code_source'])\n",
    "                                     if x['next_round'] == x['round']\n",
    "                                     and x['next_username'] == x['username']\n",
    "                                     and x['next_year'] == x['year']\n",
    "                                     else 0.0, axis=1)\n",
    "    # rearranging fields \n",
    "    df = df[df['string_distance']<0.9]\n",
    "    df = df[['username', 'year', 'round', 'task', 'code_len', 'code_lang', 'code_source', 'next_code_cut', 'string_distance']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = features_preprocessing(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_path = os.path.join(os.path.dirname(os.getcwd()),'raw_data/preprocessed_dataset.csv')\n",
    "dfp.to_csv(preproc_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('xref')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f7247b18b764b728c9316c8e8211e2dd77730fa815d30a544d828034fc29a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
